{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise_9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bPNSqGZY2kAP"
      },
      "source": [
        "# Exercise 9: Principal and Independent Component Analysis (PCA & ICA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WulkMzpA21dA"
      },
      "source": [
        "**Note**: Please insert the names of all participating students:\n",
        "1. \n",
        "2. \n",
        "3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MHw9w8GD24Jy"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise. This mandatory preamble will be found on all exercise sheets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d_YTfCfY2kAV",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-uni-luebeck/cs4405.git'\n",
        "    os.chdir('cs4405')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import utils_9 as utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wr9vrg_t2kAS"
      },
      "source": [
        "## Exercise 9.1: Principal Component Analysis (PCA)\n",
        "\n",
        "*Principal Component Analysis (PCA)* is a procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
        "\n",
        "Let $\\boldsymbol{X} \\in \\mathbb{R}^{L \\times d}$ be a given data matrix that contains $L$ `samples` $\\boldsymbol{x}_{i} \\in \\mathbb{R}^{1 \\times d}$, $i = 1,...,L$.\n",
        "The principal components of $\\boldsymbol{X}$ can be derived by computing the `eigenvectors` $\\boldsymbol{W} \\in \\mathbb{R}^{d \\times d}$ and `eigenvalues` $\\boldsymbol{v} \\in \\mathbb{R}^{1 \\times d}$ of the sample `covariance` matrix\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\boldsymbol{C}_{\\boldsymbol{X}} = \\frac{1}{L-1} \\sum_{i=1}^{L} \\left(\\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{\\boldsymbol{X}}\\right)^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{\\boldsymbol{X}} \\right)\n",
        "\\end{equation},\n",
        "$$\n",
        "\n",
        "with the `sample_mean` $\\boldsymbol{\\mu}_{\\boldsymbol{X}} \\in \\mathbb{R}^{1 \\times d}$\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\boldsymbol{\\mu}_{\\boldsymbol{X}} = \\frac{1}{L}\\sum_{i=1}^{L} \\boldsymbol{x}_{i}\n",
        "\\end{equation}.\n",
        "$$\n",
        "\n",
        "By performing a change of basis, the mean-free samples $\\boldsymbol{\\tilde{X}}$ can be represented within the obtained (principal component) basis $\\boldsymbol{W}$ as follows:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "  \\boldsymbol{\\hat{X}} = \\boldsymbol{\\tilde{X}} \\boldsymbol{W}\n",
        "\\end{equation}.\n",
        "$$\n",
        "\n",
        "**Tasks**:\n",
        "- Implement the function `principal_component_analysis` that computes the principal components of a given RGB color image and represents the image within the obtained (principal component) basis.\n",
        "\n",
        "**Programming Hints**:\n",
        "- An RGB image $\\boldsymbol{I} \\in \\mathbb{R}^{h \\times w \\times 3}$ can also be represented as $\\boldsymbol{I'} \\in \\mathbb{R}^{h \\cdot w \\times 3}$ by flattening the first two dimensions.\n",
        "- The first principal component corresponds to the `eigenvector` with the largest `eigenvalue`.\n",
        "- The NumPy functions `np.linalg.eig` and `np.argsort` might by helpful.\n",
        "\n",
        "**Questions**:\n",
        "1. What is the relation between the `eigenvalues` and the marginal sample variances after the transformation?\n",
        "2. Can you think of a reasonable interpretation of the image \"color\" channels after the transformation?\n",
        "\n",
        "**Answers**:\n",
        "1. \n",
        "2. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cxLbfExJ2kAo",
        "colab": {}
      },
      "source": [
        "def principal_component_analysis(image):\n",
        "  \n",
        "  # store the image shape\n",
        "  shape = image.shape\n",
        "\n",
        "  # TODO: flatten the spatial dimensions of the image\n",
        "  samples = \n",
        "\n",
        "  # n_samples: number of samples / n_features: number of features\n",
        "  n_samples, n_features = samples.shape\n",
        "\n",
        "  # TODO: subtract the sample mean\n",
        "  samples = \n",
        "\n",
        "  # TODO: calculate the covariance matrix\n",
        "  covariance = \n",
        "\n",
        "  # TODO: calculate the eigenvalues and eigenvectors (principal components)\n",
        "  eigenvalues, eigenvectors = \n",
        "\n",
        "  # TODO: sort the eigenvectors based on the eigenvalues in descending order\n",
        "  eigenvectors = \n",
        "\n",
        "  # TODO: perform a change of basis\n",
        "  samples = \n",
        "\n",
        "  return np.reshape(samples, shape)\n",
        "\n",
        "\n",
        "_, image = utils.load_data('data/data_9.npz')\n",
        "utils.show_image(image)\n",
        "image_pca = principal_component_analysis(image)\n",
        "utils.show_image(image_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZlRbEwV2kAz"
      },
      "source": [
        "## Exercise 9.2: Independent Component Analysis (ICA)\n",
        "This exercise considers the *Independent Component Analysis (ICA)*. While the principal components in PCA are only uncorrelated, the independent components in ICA are also statistically independent.\n",
        "\n",
        "To perform the ICA the following steps are necessary:\n",
        "\n",
        "1. Compute and subtract the `sample_mean` of the `samples`.\n",
        "2. Remove correlations from the data by performing a `principal_component_analysis` and a change of basis.\n",
        "3. Now, correlations have been removed from the data but the dimensions do not have the same variance. In order to align the variances of the dimensions, each dimension is divided by its standard deviation. This step is also called *whitening*.\n",
        "4. The last step tries to maximize the statistical independence of the data dimensions by rotating the whitened data $\\boldsymbol{X} \\in \\mathbb{R}^{L \\times d}$ as follows: \n",
        "$$\n",
        "\\begin{equation}\n",
        "  \\boldsymbol{Y} = \\boldsymbol{X} \\boldsymbol{A}_{\\theta}^{T}\n",
        "\\end{equation},\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\boldsymbol{A}_{\\theta} = \\left(\n",
        "        \\begin{matrix}\n",
        "            \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
        "            \\sin(\\theta)  & \\cos(\\theta)\n",
        "        \\end{matrix}\n",
        "    \\right)\n",
        "\\end{equation}\n",
        "$$\n",
        "is a `rotation_matrix` w.r.t. an `angle` $\\theta$, such that the non-Gaussianity of the marginal data distributions is maximal, i.e., until the marginal distributions are as dissimilar to a Gaussian distribution as\n",
        "possible. Utilize the `kurtosis` to measure the `non_gaussianity`, i.e., (iteratively) find the rotation\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\theta^{*} = \\underset{\\theta}{\\operatorname{argmax}}\\left(\n",
        "        \\left\\vert \n",
        "            3 - \\frac{1}{d} \\sum_{i=1}^{d} \\text{kurtosis}(\\boldsymbol{y}_i)\n",
        "        \\right\\vert\n",
        "    \\right)\n",
        "\\end{equation}.\n",
        "$$\n",
        "where $\\boldsymbol{y}_{i} \\in \\mathbb{R}^{1 \\times L}$ is the i-th column of $\\boldsymbol{Y}$.\n",
        "\n",
        "Often ICA is used in order to perform *Blind Source Separation (BSS)*: Given a linear mixture of source signals (e.g. audio, images), BSS aims to reconstruct the original source signals from the recorded mixtures. This problem also is known as the *Cocktail Party Problem*, i.e., we want to obtain the separated statements of all participants of a recorded cocktail party.\n",
        "\n",
        "**Tasks**:\n",
        "- Implement the function `independent_component_analysis` that tries to reconstruct the original images from the mixture of two grayscale images.\n",
        "\n",
        "**Programming Hints**:\n",
        "- Reuse the function `principal_component_analysis` from exercise 9.1, which returns an uncorrelated image $\\boldsymbol{I} \\in \\mathbb{R}^{h \\times w \\times 3}$ with zero mean.\n",
        "- The functions `utils.rotation_matrix` and `utils.kurtosis` might be helpful.\n",
        "- Use broadcasting and try to avoid for-loops.\n",
        "\n",
        "**Questions**:\n",
        "- Why do we look for the rotation that leads to maximal non-Gaussianity in the resulting data dimensions?\n",
        "\n",
        "**Answers**:\n",
        "- \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9dY1CnsW2kA2",
        "colab": {}
      },
      "source": [
        "def independent_component_analysis(image):\n",
        "  \n",
        "  # mean subtraction, pca, and change of basis\n",
        "  image_pca = principal_component_analysis(image)\n",
        "\n",
        "  # TODO: perform the whitening\n",
        "  image_whitened = \n",
        "\n",
        "  # maximize the statistical independence\n",
        "  max_non_gaussianity = 0\n",
        "  for angle in range(180):\n",
        "    \n",
        "    # TODO: calculate the rotation matrix\n",
        "    rotation_matrix = \n",
        "\n",
        "    # TODO: rotate the whitened image\n",
        "    image_rotated = \n",
        "\n",
        "    # calculate the kurtosis of the rotated image\n",
        "    kurtosis = \n",
        "\n",
        "    # calculate the non-gaussianity\n",
        "    non_gaussianity = \n",
        "        \n",
        "    if non_gaussianity > max_non_gaussianity:\n",
        "      max_non_gaussianity = non_gaussianity\n",
        "      image_ica = image_rotated\n",
        "\n",
        "  return image_ica\n",
        "\n",
        "\n",
        "image_1, image_2 = utils.load_data('data/data_9.npz',\n",
        "                                   grayscale=True)\n",
        "image = utils.mix_images(image_1, image_2)\n",
        "utils.show_image(image)\n",
        "image_ica = independent_component_analysis(image)\n",
        "utils.show_image(image_ica)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}