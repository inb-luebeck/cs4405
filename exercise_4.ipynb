{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fG14zO6gSXw1"
      },
      "source": [
        "# Exercise 4: Learning with a Multilayer Perceptron (MLP)\n",
        "In this exercise, we will study a multilayer perceptron (MLP) with one hidden layer (comprising $M$ hidden neurons) and a single output neuron.\n",
        "\n",
        "We obtain the output of the MLP through *forward propagation* as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\boldsymbol{v}_{i} &= \\sigma_{\\beta} \\left( \\boldsymbol{\\hat{x}}_{i} \\boldsymbol{\\hat{W}}_{h}^{T}  \\right) \\\\\n",
        "    y_{i} &= \\sigma_{\\beta} \\left( \\boldsymbol{\\hat{v}}_{i} \\boldsymbol{\\hat{w}}_{o}^{T} \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where \n",
        "- $\\boldsymbol{\\hat{x}}_{i} \\in \\mathbb{R}^{1 \\times (N + 1)}$ is the extended `sample` $\\boldsymbol{x}_{i} \\in \\mathbb{R}^{1 \\times N}$,\n",
        "- $\\boldsymbol{\\hat{W}}_{h} \\in \\mathbb{R}^{M \\times (N + 1)}$ are the threshold extended `hidden_weights` of the hidden neurons,\n",
        "- $\\boldsymbol{v}_{i} \\in \\mathbb{R}^{1 \\times M}$ are the $M$ `hidden_outputs` of the hidden neurons, \n",
        "- $\\boldsymbol{\\hat{v}}_{i} \\in \\mathbb{R}^{1 \\times (M + 1)}$ is the extended hidden layer output vector,\n",
        "- $\\boldsymbol{\\hat{w}}_{o} \\in \\mathbb{R}^{1 \\times (M + 1)}$ are the threshold extended `output_weights` of the output neuron,\n",
        "- $y_{i} \\in \\mathbb{R}$ is the scalar `output` of the output neuron,\n",
        "- $\\sigma_{\\beta} \\left(\\cdot\\right) = \\text{tanh}\\left(\\frac{\\beta}{2}\\cdot\\right)$ is the perceptron `activation_function`.\n",
        "\n",
        "**Note**: The _threshold trick_ is applied, i.e. the threshold of each neuron is included as an additional _first_ component for each extended weight vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6MBlbv-LSXwl"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise. This mandatory preamble will be found on all exercise sheets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gcKtz6k2SXwn"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-luebeck/cs4405.git'\n",
        "    os.chdir('cs4405')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import utils_4 as utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x19ojfyPSXw5"
      },
      "source": [
        "## Exercise 4.1: Implementation of the Backpropagation Algorithm\n",
        "Now we want to learn the `hidden_weights` $\\boldsymbol{\\hat{W}}_h$ and the `output_weights` $\\boldsymbol{\\hat{w}}_o$ from training data. Assuming that $\\beta=2$, i.e. $\\sigma_2\\left(\\cdot\\right)=\\text{tanh}\\left(\\cdot \\right)$, the backpropagation rule defined as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\delta_o &= \\left( s_i - y_i \\right) \\cdot \\left( 1 - y_i^{2} \\right) \\\\\n",
        "    \\boldsymbol{\\delta}_{h} &= \\delta_o \\cdot \\boldsymbol{w}_{o} \\odot \\left( 1 - \\boldsymbol{v}_i \\odot \\boldsymbol{v}_i \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $s$ is the `label` of an input $\\boldsymbol{x}$ and $\\odot$ is the element-wise Hadamard product. Finally, the learning rule including the `learning_rate` $\\varepsilon$ for the weight update is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\Delta \\boldsymbol{\\hat{w}}_{o} &= \\varepsilon \\cdot \\delta_o \\cdot \\boldsymbol{\\hat{v}}_i \\\\\n",
        "    \\Delta \\boldsymbol{\\hat{w}}_{h} &= \\varepsilon \\cdot \\boldsymbol{\\delta}^{T}_{h} \\cdot \\boldsymbol{\\hat{x}}_i.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Tasks**:\n",
        "- Implement the backpropagation algorithm of the MLP.\n",
        "\n",
        "**Preparation**:\n",
        "- Formulate the backpropagation algorithm in pseudocode. You should particularly pay attention to the order in which computations are performed. For every variable, specify its dimension (see also the programming hints below).\n",
        "\n",
        "**Programming Hints**:\n",
        "- Note: The source code template expects the thresholds of the hidden layer neurons and the output neuron to be in the _first_ component.\n",
        "- In each adaptation of the weight vectors a fix `learning_rate` $\\varepsilon \\in \\mathbb{R}^{+}$ has to be employed.\n",
        "- As noted above, we need to use $\\beta=2$. This means that the MLP does not output a discrete classification ($-1$ or $1$) but a real value $y_i \\in \\left[ -1;1 \\right]$.\n",
        "- You should care about the treatment of the threshold $\\theta$ in particular. Note that $\\theta$ is necessary for forward propagation but not for backpropagation, i.e., for computing the $\\delta$ values.\n",
        "- You can implement both the backpropagation rule and the learning rule without any for-loops (`help(np.outer)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d1IUm2pmSXw8"
      },
      "outputs": [],
      "source": [
        "def learn_mlp(samples, labels, hidden_neurons, learning_rate, epochs):\n",
        "  \n",
        "  # n_samples: number of training samples / n_features: number of features\n",
        "  n_samples, n_features = samples.shape\n",
        "\n",
        "  # initialize the weights for the hidden layer (threshold trick included)\n",
        "  hidden_weights = np.random.uniform(low=-1,\n",
        "                                     high=1,\n",
        "                                     size=(hidden_neurons, n_features + 1))\n",
        "  \n",
        "  # initialize the weights for the output layer (threshold trick included)\n",
        "  output_weights = np.random.uniform(low=-1,\n",
        "                                     high=1,\n",
        "                                     size=(hidden_neurons + 1,))\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    # generate randomly permuted index array\n",
        "    indexes = np.random.permutation(n_samples)\n",
        "    \n",
        "    # iterate through all indexes in the index array\n",
        "    for index in indexes:\n",
        "      \n",
        "      # select training sample and corresponding class label according to generated random permutation\n",
        "      sample = samples[index]\n",
        "      label = labels[index]\n",
        "\n",
        "      # forward propagation\n",
        "      output, hidden_outputs = utils.classify_mlp(sample, hidden_weights, output_weights, beta=2)\n",
        "\n",
        "      # was the data point classified incorrectly?\n",
        "      if (label * output) < 0:\n",
        "        \n",
        "        # TODO: implement the backpropagation rule\n",
        "        delta_output = \n",
        "        delta_hidden = \n",
        "\n",
        "        # extend tensors with '-1' dimension as first component (threshold trick)\n",
        "        sample = utils.extend(sample)\n",
        "        hidden_outputs = utils.extend(hidden_outputs)\n",
        "\n",
        "        # TODO: implement the learning rule for the weight updates\n",
        "        output_weights = \n",
        "        hidden_weights = \n",
        "        \n",
        "    yield {'hidden_weights': hidden_weights, \n",
        "           'output_weights': output_weights}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oi0qYPwDSXxI"
      },
      "source": [
        "## Exercise 4.2: Training with an MLP\n",
        "The following two training data sets are given:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "faVkscPiSXxK"
      },
      "outputs": [],
      "source": [
        "samples, labels = utils.load_data('data/data_4_1.npz')\n",
        "plt.subplot(1, 2, 1)\n",
        "utils.plot_data(samples, labels)\n",
        "samples, labels = utils.load_data('data/data_4_2.npz')\n",
        "plt.subplot(1, 2, 2)\n",
        "utils.plot_data(samples, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CnIW7v7ZSXxV"
      },
      "source": [
        "**Tasks**:\n",
        "- We want to use an MLP to correctly classify these data sets. \n",
        "- Obviously, this is not possible using a single artificial neuron since the data are not linearly separable.\n",
        "- Try to let your MLP learn both structures as well as possible. \n",
        "\n",
        "**Questions**:\n",
        "- How many hidden neurons and which learning rate are required in order to successfully classify each structure?\n",
        "\n",
        "**Answers**:\n",
        "- \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GPZJqFVTSXxW"
      },
      "outputs": [],
      "source": [
        "hidden_neurons = 4\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "samples, labels = utils.load_data('data/data_4_1.npz')\n",
        "generator = learn_mlp(samples, labels, hidden_neurons, learning_rate, epochs)\n",
        "animation = utils.Animation(samples, labels, hidden_neurons)\n",
        "animation.animate(generator,\n",
        "                  max_frames=100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "exercise_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
