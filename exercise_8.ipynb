{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da020cb8",
   "metadata": {
    "id": "da020cb8"
   },
   "source": [
    "# Exercise 8: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed0a2d",
   "metadata": {
    "id": "70ed0a2d"
   },
   "source": [
    "## Preamble\n",
    "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise. This mandatory preamble will be found on all exercise sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41093d",
   "metadata": {
    "id": "3c41093d"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "  if os.getcwd() == '/content':\n",
    "    !git clone 'https://github.com/inb-luebeck/cs4405.git'\n",
    "    os.chdir('cs4405')\n",
    "\n",
    "!pip install transformers==4.47.1 accelerate\n",
    "from utils import utils_8 as utils\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "import torch\n",
    "from torch import nn\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "phi3config = utils.get_config()\n",
    "tokenizer = utils.get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d159b89",
   "metadata": {
    "id": "6d159b89"
   },
   "source": [
    "## Note\n",
    "The model implementation used in this exercise is based on [Microsoft's Phi-3](https://arxiv.org/abs/2404.14219).\n",
    "\n",
    "The model code in this exercise includes blocks that are not essential for a fundamental understanding of the transformer architecture. Those blocks are marked with `### OPTIMIZATION` and can be ignored by the reader for a better overview of the model at large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a7ddd",
   "metadata": {
    "id": "a44a7ddd"
   },
   "source": [
    "# Language Modeling Basics\n",
    "## Tokenization\n",
    "The input of a (large) language model (LLM) is a sequence of word/token IDs, which are their indices in a predefined dictionary. Likewise, a sequence can be decoded back into text by looking up the words/word fragments corresponding to the IDs and concatenating them.\n",
    "\n",
    "The dictionary tokens are defined based on the frequency of that character sequence in a given language or dataset.\n",
    "\n",
    "Here is a demonstration for tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69770846",
   "metadata": {
    "id": "69770846",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = \"A simplified example for tokenization\"\n",
    "\n",
    "token_ids = tokenizer(text, add_special_tokens=False).input_ids\n",
    "print(\"Token IDs:\", token_ids, \"\\n\")\n",
    "\n",
    "# Decoding tokens one by one to show which word fragments are encoded as individual tokens\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"List of decoded tokens:\", decoded_tokens, \"\\n\")\n",
    "\n",
    "# Decoding the text as a whole from the token ID sequence\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded text: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ae9b3",
   "metadata": {
    "id": "da8ae9b3"
   },
   "source": [
    "## Embeddings\n",
    "For every entry in the token dictionary the model holds a trainable representation vector. These vectors are initialized with random values, just like other neural network parameters, and are trained with the rest of the model via backpropagation.\n",
    "\n",
    "The trainable representation vectors are called embeddings and they are stored in a matrix (we will call it the embedding matrix), with each row corresponding to one token ID (one entry in the token dictionary). The embedding matrix can be interpreted as a lookup table.\n",
    "\n",
    "For this LLM, we use:\n",
    " - a tokenizer with a dictionary size of $32064$ tokens\n",
    " - embedding vectors of dimension $3072$\n",
    "\n",
    "## Model Output\n",
    "For the language modeling task, the model is trained to predict the next token. The prediction gives every token in the dictionary a score based on its likelihood. This is treated like a classification where each token in the dictionary is interpreted as a class, and the ground truth label is the actual next token ID in the sequence.\n",
    "\n",
    "In the case of decoder-only transformer-based language models (like the model used in this exercise) the model simultaneously produces a prediction for **every** token in the input sequence. I.e. the $i$-th vector in the output list contains the probability distribution for the $(i+1)$-th token, $i\\le n$ for an input sequence of length $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caceea6",
   "metadata": {
    "id": "2caceea6"
   },
   "source": [
    "## Transformers\n",
    "Transformer network architectures use one or more transformer blocks in sequence. Each transformer block takes a list of feature vectors $[in_0, in_1, \\dots, in_n]$ as input and produces a list (with the same length) of new feature vectors as output $[out_0, out_1, \\dots, out_n]$. Every vector of the output list is a new representation of the input vector at the same position in the list, but its representation is enriched with relational information between it and the other feature vectors.\n",
    "\n",
    "The input feature vectors $[in_0, in_1, \\dots, in_n]$ for the first transformer block are semantic units from the input sample, e.g. words or tokens in a text or image regions/patches in an image, or different images from an image series or video.\n",
    "\n",
    "The new representations can be modeled via a transformation function $f$ such that $out_i = f(in_i, [in_0, in_1, \\dots, in_n])$. The first argument, $in_i$, is used to produce a `query` and the second argument serves as `keys` and `values`. The intuition being that the `query` vector states which kind of information we are seeking in the other feature vectors, the `keys` encoding what information can be found in a feature vector, and the `values` representing the detailed information.\n",
    "\n",
    "For the transformer architecture the transformation function $f$ is modeled via self-attention or cross-attention (see lecture notes) to determine relevant information from other feature vectors for a feature vector $in_i$, followed by a multi-layer-perceptron (MLP) to process how the feature vectors are related and how to represent that relation in a new representation $out_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a711b12",
   "metadata": {
    "id": "6a711b12"
   },
   "source": [
    "### Self-Attention\n",
    "For a list of feature vectors $[in_0, in_1, \\dots, in_n]$ query, key and value vectors are calculated via a transformation matrices $W_Q$, $W_K$ and $W_V$ as follows:\n",
    "$$q_i = W_Q\\times in_i,$$\n",
    "\n",
    "$$k_i = W_K\\times in_i,$$\n",
    "\n",
    "$$v_i = W_K\\times in_i.$$\n",
    "\n",
    "The attention calculation looks as follows:\n",
    "\n",
    "$$y_i = \\sum_j softmax_j\\left(\\frac{q_i\\times k_0}{\\sqrt{keydim}}, \\frac{q_i\\times k_1}{\\sqrt{keydim}}, \\dots, \\frac{q_i\\times k_n}{\\sqrt{keydim}}\\right)\\times v_j$$\n",
    "\n",
    "where $keydim$ is the dimensionality of the key vectors.\n",
    "\n",
    "We call $\\frac{q_i\\times k_j}{\\sqrt{keydim}}$ attention scores, as they determine how much input $in_j$ contributes to the new representation of input $in_i$.\n",
    "\n",
    "We can arrange the input feature vectors as row vectors in a matrix $X$ to calculate all queries in a single matrix multiplication like so:\n",
    "\n",
    "$$Q = W_QX^T,$$\n",
    "\n",
    "$$K = W_KX^T,$$\n",
    "\n",
    "$$V = W_VX^T.$$\n",
    "\n",
    "The attention calculation then corresponds to:\n",
    "\n",
    "$$softmax\\left(\\frac{QK^T}{\\sqrt{keydim}}\\right)V$$\n",
    "\n",
    "with softmax along the rows.\n",
    "\n",
    "### Multi-Head Attention\n",
    "In practice, the whole attention mechanism is applied 32 times simultaneously (i.e. 32 attention heads), each with their own transformation matrices for $W_Q, W_K, W_V$, so that some of the attention calculations can represent different relations between the tokens than others. For example, one might focus on grammatical relations between words while others might focus on phonetical similarities between words, if the training data included poems and lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c471ab3",
   "metadata": {
    "id": "6c471ab3"
   },
   "source": [
    "## Exercise 8.1: Transformer Implementation\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "Implement a self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07946acc",
   "metadata": {
    "id": "07946acc"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MySingleSelfAttentionLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        feature_dim = 3072\n",
    "\n",
    "        # dimensionality per attention head\n",
    "        self.head_dim = 96\n",
    "\n",
    "        # TODO: create the projection matrices for the queries, keys and values (using nn.Linear without bias) - use head_dim for the dimensionality of the resulting vectors\n",
    "        self.q_proj =\n",
    "        self.k_proj =\n",
    "        self.v_proj =\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # TODO: calculate the queries, keys and values\n",
    "        queries =\n",
    "        keys =\n",
    "        values =\n",
    "\n",
    "\n",
    "        # apply position embeddings - this enriches the query and key vectors with information about their position within the input sequence\n",
    "        queries, keys = utils.apply_pos_emb(hidden_states, queries, keys, values)\n",
    "\n",
    "\n",
    "        # TODO: calculate all attention scores using matrix multiplication\n",
    "        attn_weights =\n",
    "\n",
    "\n",
    "        # applying a causal attention mask, i.e. offsetting attention scores to -inf wherever the key token comes later in the sentence/text than the query token, so that the prediction model can't predict the next word by just looking at it (instead of infering it from the prior text)\n",
    "        attention_mask = utils.get_attention_mask(phi3config, hidden_states)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "\n",
    "        # apply softmax to get a weighted sum with a total weight of 1.0\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(values.dtype)\n",
    "\n",
    "\n",
    "        # TODO: calculate the weighted sum of value vectors based on the attention they received\n",
    "        attn_output =\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b851a",
   "metadata": {
    "id": "ee0b851a"
   },
   "outputs": [],
   "source": [
    "# Sanity check to verify matching dimensions\n",
    "def sanity_check_single():\n",
    "  test_layer = MySingleSelfAttentionLayer().cpu()\n",
    "  test_input = torch.rand(4, 5, 3072, device='cpu')\n",
    "  test_output = test_layer(test_input)\n",
    "  assert list(test_output.shape) == [test_input.shape[0], test_input.shape[1], test_input.shape[2]//32], \"Tensor shape mismatch! Expected output tensor shape to match the input tensor's shape(/32)!\"\n",
    "\n",
    "sanity_check_single()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88dd58",
   "metadata": {
    "id": "8b88dd58"
   },
   "source": [
    "### Multi-Head Attention\n",
    "Use multiple attention layers to implement a multi-head self attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9043391",
   "metadata": {
    "id": "a9043391"
   },
   "outputs": [],
   "source": [
    "class MySelfAttentionLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        feature_dim = 3072\n",
    "\n",
    "        # multi-head attention, head count\n",
    "        num_heads = 32\n",
    "\n",
    "        # TODO: create 32 attention heads (MySingleSelfAttentionLayer) in a list, wrap it with nn.ModuleList\n",
    "        self.attention_heads =\n",
    "\n",
    "\n",
    "        ### OPTIMIZATION ###\n",
    "        self.o_proj = nn.Linear(feature_dim, feature_dim, bias=False)\n",
    "        ####################\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # TODO: calculate the self-attention for every attention head and concatenate their result vectors\n",
    "        attn_outputs =\n",
    "\n",
    "\n",
    "        ### OPTIMIZATION ###\n",
    "        # combine results from different attention heads using a linear layer\n",
    "        attn_outputs = self.o_proj(attn_outputs)\n",
    "        ####################\n",
    "\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to verify matching dimensions\n",
    "def sanity_check_multi():\n",
    "  test_layer = MySelfAttentionLayer().cpu()\n",
    "  test_input = torch.rand(4, 5, 3072, device='cpu')\n",
    "  test_output = test_layer(test_input)\n",
    "  assert list(test_output.shape) == list(test_input.shape), \"Tensor shape mismatch! Expected output tensor shape to match the input tensor's shape!\"\n",
    "\n",
    "sanity_check_multi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c149415",
   "metadata": {
    "id": "6c149415"
   },
   "source": [
    "### Transformer Block\n",
    "\n",
    "Implement a transformer block by combining a self attention layer with a MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af4e78",
   "metadata": {
    "id": "26af4e78"
   },
   "outputs": [],
   "source": [
    "# This is where we implement a decoder-only transformer block - the most common architecture among state-of-the-art LLMs\n",
    "class MyTransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: create a self attention layer (MySelfAttentionLayer - implementation above)\n",
    "        self.self_attn =\n",
    "        # TODO: create an MLP (MyMLP - implementation below)\n",
    "        self.mlp =\n",
    "\n",
    "        ### OPTIMIZATION ###\n",
    "        # layer normalization was used during training for better training behavior\n",
    "        self.input_layernorm = utils.Phi3RMSNorm(phi3config.hidden_size, eps=phi3config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = utils.Phi3RMSNorm(phi3config.hidden_size, eps=phi3config.rms_norm_eps)\n",
    "        ####################\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        ### OPTIMIZATION ###\n",
    "        # keep a copy of the original input vectors\n",
    "        residual = hidden_states\n",
    "        # applying layer normalization\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        ####################\n",
    "\n",
    "\n",
    "        # TODO: calculate multi-head self attention\n",
    "        attn_outputs =\n",
    "\n",
    "\n",
    "        ### OPTIMIZATION ###\n",
    "        # combine each input vector's previous representation with the most attended related vectors\n",
    "        hidden_states = residual + attn_outputs\n",
    "        # prepare the skip connection around the MLP\n",
    "        residual = hidden_states\n",
    "        # applying another layer normalization\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        ####################\n",
    "\n",
    "\n",
    "        # TODO: apply the MLP to calculate relations between each input vector's previous representation and its most attended related vectors\n",
    "        hidden_states =\n",
    "\n",
    "\n",
    "        ### OPTIMIZATION ###\n",
    "        # apply skip connection around MLP\n",
    "        hidden_states = residual + hidden_states\n",
    "        ####################\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# A self-gated MLP\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_up_proj = nn.Linear(phi3config.hidden_size, 2 * phi3config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(phi3config.intermediate_size, phi3config.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        up_states = self.gate_up_proj(hidden_states)\n",
    "\n",
    "        gate, up_states = up_states.chunk(2, dim=-1)\n",
    "        up_states = up_states * utils.silu(gate)\n",
    "\n",
    "        return self.down_proj(up_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c82be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to verify matching dimensions\n",
    "def sanity_check_block():\n",
    "  test_layer = MyTransformerBlock().cpu()\n",
    "  test_input = torch.rand(4, 5, 3072, device='cpu')\n",
    "  test_output = test_layer(test_input)[0]\n",
    "  assert list(test_output.shape) == list(test_input.shape), \"Tensor shape mismatch! Expected output tensor shape to match the input tensor's shape!\"\n",
    "\n",
    "sanity_check_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafa7d9",
   "metadata": {
    "id": "6bafa7d9"
   },
   "source": [
    "### Transformer Model\n",
    "\n",
    "Stack 32 transformer blocks and add a final linear layer that outputs a score for every token ID in the dicitonary.\n",
    "\n",
    "For more details, see the sections about **Embeddings** and **Model Output** above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519046aa",
   "metadata": {
    "id": "519046aa"
   },
   "outputs": [],
   "source": [
    "class MyModel(PreTrainedModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(phi3config)\n",
    "        # TODO: create the embedding matrix (nn.Embedding(...))\n",
    "        self.embed_tokens =\n",
    "\n",
    "\n",
    "        # TODO: make a list of 32 transformer blocks (MyTransformerBlock, implementation above) that we will propagate the inputs through\n",
    "        # NOTE: wrap the list in \"nn.ModuleList(my_list)\" to inform pytorch that these are neural network components (that hold model parameters) and not just a list of python variables\n",
    "        self.layers =\n",
    "\n",
    "        ### OPTIMIZATION ###\n",
    "        # layer normalization was used during training for better training behavior\n",
    "        self.norm = utils.Phi3RMSNorm(phi3config.hidden_size, eps=phi3config.rms_norm_eps)\n",
    "        ####################\n",
    "\n",
    "        # TODO: create a linear layer (without a bias) that maps the representations produced by the last transformer block onto the dictionary (giving every token ID a likelihood score)\n",
    "        # NOTE: as we will see below, we model every transformer block to produce outputs with the same hidden dimension as its inputs - so the output dimension of the last transformer block is still the same as the dimension of the input of the first transformer block, i.e. equal to the embedding dimension\n",
    "        self.lm_head =\n",
    "\n",
    "    def forward(self, input_ids, return_dict=False, **kwargs):\n",
    "        # TODO: get the embedding vectors for the input token ids\n",
    "        inputs_embeds =\n",
    "\n",
    "        # out initial hidden representations are the token embeddings\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # propagate through all transformer blocks sequentially\n",
    "        for transformer_block in self.layers:\n",
    "            # TODO: get the new hidden representations by transforming the current hidden representations\n",
    "            layer_outputs =\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "\n",
    "        ### OPTIMIZATION ###\n",
    "        # applying layer normalization\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        ####################\n",
    "\n",
    "\n",
    "        # TODO: calculate the next token likelihood scores from the last hidden representations using the linear mapping layer\n",
    "        logits =\n",
    "        logits = logits.float()\n",
    "\n",
    "        return utils.format_model_output(logits, return_dict)\n",
    "\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs):\n",
    "        model_inputs = {\"input_ids\": input_ids}\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to verify matching dimensions\n",
    "def sanity_check_model():\n",
    "  test_layer = MyModel().cuda()\n",
    "  test_input = torch.randint(0, phi3config.vocab_size, (4, 5), device='cuda')\n",
    "  test_output = test_layer(test_input)[0]\n",
    "  assert list(test_output.shape) == [test_input.shape[0], test_input.shape[1], phi3config.vocab_size], \"Tensor shape mismatch! Expected output tensor shape to match the input tensor's shape and dictionary size!\"\n",
    "\n",
    "sanity_check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0d8b2",
   "metadata": {
    "id": "dbe0d8b2"
   },
   "source": [
    "### Comprehension Questions\n",
    "Retrieve the shape of the tensor that goes into the first transformer block.\n",
    " - What is the meaning of each of those dimensions?\n",
    " - How can we use the output of the model? Give an example with a demonstration.\n",
    " - What does the softmax function look like, mathematically? And how does an attention score of negative infinity contribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d115e",
   "metadata": {
    "id": "a86d115e"
   },
   "source": [
    "## Exercise 8.2: Decoding the Model Output\n",
    "\n",
    "### Setup\n",
    "\n",
    "This exercise can utilize GPU acceleration. If you are using Google Colab you can enable access to a cloud GPU by selecting from the menu above:\n",
    "\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU**\n",
    "\n",
    "If you are running this notebook on your own machine, GPU acceleration is available if you have an Nvidia GPU and a CUDA-enabled driver installed.\n",
    "\n",
    "### Model Instantiation\n",
    "\n",
    "Create an instance of your model class in the following code block. The code also loads pretrained weights for the model and inserts them into the model's layers, so that we can try the model out without having to train it ourselfes (which would take an excessive amount of time and ressources for large language models).\n",
    "\n",
    "Once your model is confirmed to work and generated plausible outputs, the method below can also be used to instantiate a better optimized implementation of the same model for faster processing (by providing `use_optimized_implementation=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bdf90",
   "metadata": {
    "id": "782bdf90"
   },
   "outputs": [],
   "source": [
    "# Helper code to create a mode instance\n",
    "def create_model_instance(use_optimized_implementation=False):\n",
    "    # with use_optimized_implementation=False this function uses our model implementation\n",
    "    # with use_optimized_implementation=True a more optimized implementation is used\n",
    "\n",
    "    if not use_optimized_implementation:\n",
    "        torch.set_default_device('cuda')\n",
    "        torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "        # retrieving pretrained weights\n",
    "        model_weights = utils.get_model_weights()\n",
    "\n",
    "        # TODO: create an instance of your model\n",
    "        model =\n",
    "\n",
    "        # loading pretrained weights into the model\n",
    "        model.load_state_dict(model_weights)\n",
    "    else:\n",
    "        model = utils.get_model()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdf590",
   "metadata": {
    "id": "ebcdf590"
   },
   "source": [
    "### A single Forward Pass\n",
    "\n",
    "Create an instance of your model using `create_model_instance`, tokenize an input text and feed the token sequence into the model to predict the next token. When selecting the next token, take the one with the highest score (i.e. greedy decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8d7e0",
   "metadata": {
    "id": "1cc8d7e0"
   },
   "outputs": [],
   "source": [
    "def experiment1():\n",
    "    # TODO: create a model instance\n",
    "    model =\n",
    "\n",
    "    # Some test input\n",
    "    input_text = \"Hello, nice to\"\n",
    "\n",
    "    # TODO: tokenize the input text\n",
    "    token_ids =\n",
    "\n",
    "    # make a pytorch tensor out of the token id list\n",
    "    token_ids_tensor = torch.tensor(token_ids)\n",
    "\n",
    "    # add a batch dimension (we only have one sequence so batch size is 1)\n",
    "    batch = token_ids_tensor.unsqueeze(dim=0)\n",
    "\n",
    "    # move the batch to the GPU\n",
    "    batch = batch.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # TODO: feed the batch into the model\n",
    "        out =\n",
    "        logits = out[0] # a model could output more data, like the attention scores, to cache them for auto-regression, but we are only interested in the logits\n",
    "\n",
    "        # TODO: the \"logits\" variable contains the prediction scores for every token's successor in the sequence, not just for the last token. Get the scores for the last token.\n",
    "        next_token_logits =\n",
    "\n",
    "        # TODO: determine the next token ID (i.e. the id/index of the token with the highest score)\n",
    "        next_token_id =\n",
    "\n",
    "        # TODO: decode the sequence with the new token attached at the end\n",
    "        new_text =\n",
    "\n",
    "        print(new_text)\n",
    "\n",
    "experiment1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb1b30",
   "metadata": {
    "id": "accb1b30"
   },
   "source": [
    "### Top k\n",
    "\n",
    "Convert the predicted token scores into token probabilities by applying softmax to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcadfeb",
   "metadata": {
    "id": "dfcadfeb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def experiment2():\n",
    "    # TODO: create a model instance\n",
    "    model =\n",
    "\n",
    "    # Some test input\n",
    "    input_text = \"Hello, nice to\"\n",
    "\n",
    "    # TODO: tokenize the input text\n",
    "    token_ids =\n",
    "\n",
    "    # make a pytorch tensor out of the token id list\n",
    "    token_ids_tensor = torch.tensor(token_ids)\n",
    "\n",
    "    # add a batch dimension (we only have one sequence so batch size is 1)\n",
    "    batch = token_ids_tensor.unsqueeze(dim=0)\n",
    "\n",
    "    # move the batch to the GPU\n",
    "    batch = batch.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # TODO: feed the batch into the model\n",
    "        out =\n",
    "        logits = out[0] # a model could output more data, like the attention scores, to cache them for auto-regression, but we are only interested in the logits\n",
    "\n",
    "        # TODO: the \"logits\" variable contains the prediction scores for every token's successor in the sequence, not just for the last token. Get the scores for the last token.\n",
    "        next_token_logits =\n",
    "\n",
    "\n",
    "        # TODO: calculate the probabilities for all token IDs (i.e. their probability to be the next token in the sequence)\n",
    "        next_token_probabilities =\n",
    "\n",
    "        # TODO: sort by probability and store the id/index corresponding to each probability\n",
    "        next_token_probabilities, next_token_ids =\n",
    "\n",
    "        k = 5\n",
    "\n",
    "        # TODO: take the top k tokens\n",
    "        top_k_probs, top_k =\n",
    "\n",
    "        # TODO: for each of the top-k entries print the decoded text and the probability\n",
    "        for p, token_id in zip(top_k_probs, top_k):\n",
    "            print(\"Probability:\", p.item() * 100)\n",
    "            text =\n",
    "            print(text, '\\n')\n",
    "\n",
    "experiment2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525186c0",
   "metadata": {
    "id": "525186c0"
   },
   "source": [
    "### Autoregression\n",
    "\n",
    "Use the model to continue the text in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5905a1",
   "metadata": {
    "id": "ec5905a1"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "class printer(str):\n",
    "    def __repr__(self):\n",
    "       return self\n",
    "\n",
    "def experiment3():\n",
    "    # TODO: create a model instance\n",
    "    model =\n",
    "\n",
    "    # Some test input\n",
    "    input_text = \"(Narrator:) Hello, nice to\"\n",
    "\n",
    "    # TODO: tokenize the input text\n",
    "    token_ids =\n",
    "\n",
    "    max_new_tokens = 10\n",
    "\n",
    "    # The token that the model uses to signify the end of the output\n",
    "    end_token_id = tokenizer.convert_tokens_to_ids(['<|end|>'])[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # make a pytorch tensor out of the token id list\n",
    "            token_ids_tensor = torch.tensor(token_ids)\n",
    "\n",
    "            # add a batch dimension (we only have one sequence so batch size is 1)\n",
    "            batch = token_ids_tensor.unsqueeze(dim=0)\n",
    "\n",
    "            # move the batch to the GPU\n",
    "            batch = batch.cuda()\n",
    "\n",
    "            # TODO: feed the batch into the model\n",
    "            out =\n",
    "            logits = out[0]\n",
    "\n",
    "            # TODO: the \"logits\" variable contains the prediction scores for every token's successor in the sequence, not just for the last token. Get the scores for the last token.\n",
    "            next_token_logits =\n",
    "\n",
    "            # TODO: determine the next token ID (i.e. the id/index of the token with the highest score)\n",
    "            next_token_id =\n",
    "\n",
    "            # TODO: add the new token to the input sequence\n",
    "            token_ids...\n",
    "\n",
    "            # show the generated text so far\n",
    "            clear_output(wait=True)\n",
    "            display(printer(tokenizer.decode(token_ids)))\n",
    "\n",
    "            if (next_token_id == end_token_id):\n",
    "                break\n",
    "\n",
    "experiment3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a188141",
   "metadata": {
    "id": "1a188141"
   },
   "source": [
    "## Additional Content\n",
    "\n",
    "The libraries used here already provide premade functions to predict and decode text in a loop.\n",
    "An example is provided below.\n",
    "\n",
    "The `temperature` parameter is simply a factor that the logits/token scores are divided by before applying softmax. It controls the variance of the generated text - the lower the temperature, the more probabilistic weight moves towards the tokens with the highest scores.\n",
    "\n",
    "The `do_sample` parameter picks the next token at random based on each token ID's predicted probability, rather than greedily always picking the one with the highest probability. This can be combined with a `top_p` parameter that can be used to filter out less likely tokens so we don't occasionally encounter very inaccurate tokens due to bad luck, or the `top_k` parameter that only keeps the k most likely tokens to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8647c",
   "metadata": {
    "id": "22a8647c"
   },
   "outputs": [],
   "source": [
    "def generation_example():\n",
    "    model = create_model_instance(True)\n",
    "\n",
    "    input_text = \"Hello, nice to\"\n",
    "\n",
    "    token_ids = tokenizer.encode(input_text)\n",
    "    output_ids = model.generate(\n",
    "        torch.tensor([token_ids]).to(model.device),\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        max_new_tokens=30,\n",
    "    )\n",
    "    new_token_ids = output_ids.tolist()[0][len(token_ids):]\n",
    "    all_token_ids = token_ids + new_token_ids\n",
    "\n",
    "    result_text = tokenizer.decode(all_token_ids, skip_special_tokens=True)\n",
    "    print(result_text)\n",
    "\n",
    "generation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded32c0",
   "metadata": {
    "id": "6ded32c0"
   },
   "source": [
    "The model was also fine tuned to continue text in a chat format. The tokenizer includes the template.\n",
    "Below you can find an example for LLMs work in a chat setting.\n",
    "\n",
    "You can also see that the model predicts an \"<|end|>\" token where it predicts the response should be finished. This is what the generating loop is looking for to terminate before `max_new_tokens` is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730ea0d",
   "metadata": {
    "id": "b730ea0d"
   },
   "outputs": [],
   "source": [
    "def chat_example():\n",
    "    model = create_model_instance(True)\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Write a short sentence about drying paint.\"},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    print(f'Prompt: \\n\"\\n{prompt}\"\\n')\n",
    "\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        token_ids,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        max_new_tokens=30,\n",
    "    )\n",
    "    new_token_ids = output_ids.tolist()[0][token_ids.shape[1]:]\n",
    "\n",
    "    response_text = tokenizer.decode(new_token_ids, skip_special_tokens=False)\n",
    "    print(f\"Model response: \\n{response_text}\")\n",
    "\n",
    "chat_example()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
