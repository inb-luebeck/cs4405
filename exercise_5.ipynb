{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Exercise 5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpB4cCtpL1nN",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 5: PyTorch and Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLMj-SUTQ9Tl",
        "colab_type": "text"
      },
      "source": [
        "**Note**: Please insert the names of all participating students:\n",
        "1. \n",
        "2. \n",
        "3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRD_ihD_RHqd",
        "colab_type": "text"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise. This mandatory preamble will be found on all exercise sheets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKuCBDgRK-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-uni-luebeck/cs4405.git'\n",
        "    os.chdir('cs4405')\n",
        "        \n",
        "#making sure idx2numpy is installed, which is used in utils\n",
        "try:\n",
        "    import idx2numpy\n",
        "except ModuleNotFoundError:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install idx2numpy\n",
        "    import idx2numpy\n",
        "\n",
        "#checking if data is unzipped and unzip if necessary\n",
        "if not os.path.isfile('data/emnist-byclass-train-images-idx3-ubyte'):\n",
        "    !zip -s- data/data_5.zip -O data/data_5_final.zip\n",
        "    !unzip data/data_5_final.zip -d data/\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import utils_5 as utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXkc4_8R-9B",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "This exercise can utilize GPU acceleration. If you are using Google Colab you can enable access to a cloud GPU by selecting from the menu above: \n",
        "\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU**\n",
        "\n",
        "If you are running this notebook on your own machine, GPU acceleration is available if you have an Nvidia GPU and a CUDA-enabled driver installed. Otherwise calculations will run on the CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oLfV93sL1nR",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch\n",
        "PyTorch is a deep learning framework developed with numpy compatibility in mind. Many operations are designed to closely resemble those in numpy, but it is optimized for larger data structures and parallel processing with support for GPU acceleration. Additionally it contains many tools to perform common tasks in machine learning like data loaders, gradient-based optimization algorithms and basic building blocks for neural networks. \n",
        "\n",
        "To use PyTorch we need to import the `torch` package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h69vHyVL1nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGYz4m4L1ng",
        "colab_type": "text"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "`torch.Tensor` is the fundamental data structure in PyTorch. The word Tensor in this context describes a numeric multidimensional array. Operations are applied to tensors and return a tensor as a result. A tensor can be created from a list, tuple or multidimendional numpy array using the `torch.tensor()` function: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtp6gc4FL1ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.array([[1,2,3],\n",
        "              [4,5,6]])\n",
        "\n",
        "b = torch.tensor(a)\n",
        "print(b.size())\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqjvI8CXL1nv",
        "colab_type": "text"
      },
      "source": [
        "Array indexing and arithmetics work the same as with numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_KFRru4L1nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding and multiplying with broadcasting\n",
        "c = b * 3 + 5\n",
        "print(c)\n",
        "\n",
        "# Element-wise add of two tensors\n",
        "d = b + c\n",
        "print(d)\n",
        "\n",
        "print(d[1,1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RoSH_B7L1oJ",
        "colab_type": "text"
      },
      "source": [
        "Additionally to a data type, tensors also contain information about which device they reside on. It can either be system memory or the memory of a GPU.\n",
        "\n",
        "__Important note__:\n",
        "Current pytorch binary releases only support GPUs with CUDA compute compatibility 3.5+. At the time of writing, pool PCs in most pools only have CUDA compute compatibility 3.0 capable hardware (which is still reported as `torch.cuda.is_available() = True` but fails on many operations with an error message). If you encounter CUDA or cuDNN related errors, manually set `use_gpu = False` here to force computations to use the CPU instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3aS4YWzL1oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#device='cpu' creates the tensor in system memory, which is also the default\n",
        "a = torch.tensor([[1, 3, 7], [2, 5, 3]], dtype=torch.float, device='cpu')\n",
        "print(a)\n",
        "\n",
        "#we can check if GPU support is available on the current machine and environment:\n",
        "print(torch.cuda.is_available())\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "if use_gpu:\n",
        "    #we can create a copy of the tensor on the default gpu\n",
        "    b = a.cuda()\n",
        "    print(b)\n",
        "    #or we can explicitly specify which device to use\n",
        "    c = a.cuda(0)\n",
        "    print(c)\n",
        "    #we can specify the gpu as target during initialization\n",
        "    d = torch.tensor([[1, 3, 7], [2, 5, 3]], dtype=torch.float, device='cuda:0')\n",
        "    print(d)\n",
        "    #and we can create a copy on the cpu\n",
        "    e = d.cpu()\n",
        "    print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sevnfSA1L1ov",
        "colab_type": "text"
      },
      "source": [
        "Operations on multiple tensors (e.g. add) require the tensors to reside on the same device.\n",
        "\n",
        "A list of basic tensor operations can be found here: https://pytorch.org/docs/stable/tensors.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qodzn9EOL1ow",
        "colab_type": "text"
      },
      "source": [
        "### Autograd\n",
        "\n",
        "PyTorch tensors keep track of how they were created and all basic operations contain information of what their derivative looks like. If we want to calculate the derivative of a result with respect to an input, we can create the input tensor with `requires_grad=True` and propagate the differentiation back to the beginning of the calculation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYV6_83hL1oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.tensor([2, 3], dtype=torch.float, requires_grad=True)\n",
        "b = torch.tensor([5, 1], dtype=torch.float, requires_grad=True)\n",
        "\n",
        "#elementwise multiplication\n",
        "c = a*b\n",
        "print(c)\n",
        "#square all elements and add them\n",
        "d = c.pow(2).sum()\n",
        "\n",
        "print(d)\n",
        "\n",
        "#calculate the derivative of d with respect to all elements of its inputs (recursively through backpropagation)\n",
        "d.backward()\n",
        "\n",
        "#the derivatives are stored in the input tensors\n",
        "print(c.grad)#is None since c was implicitly created with requires_grad=False. You might see a warning since this is an intermediate tensor, not a leaf tensor of the calculation\n",
        "print(b.grad)\n",
        "print(a.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GczlNkgxL1pA",
        "colab_type": "text"
      },
      "source": [
        "Since `d` was created through arithmetic operations of other tensors, we can interpret it as a function that was evaluated with set parameters and calculate its derivative using the chain rule: \n",
        "$$\n",
        "d = d(\\vec{c}) = d(\\vec{c}(\\vec{a}, \\vec{b}))\\\\\n",
        "\\frac{\\partial{d}}{\\partial{a_0}}{\\bigg|}_{a=[2, 3], b=[5, 1]} = (2\\cdot c_0)\\cdot b_0 |_{a=[2, 3], b=[5, 1]} = (2 \\cdot (a_0 \\cdot b_0)) \\cdot b_0|_{a=[2, 3], b=[5, 1]} = 100 = \\mathrm{a.grad[0]}\n",
        "$$\n",
        "\n",
        "So if `d` was our loss function, we could use `a.grad` and `b.grad` to adjust `a` and `b` to minimize the loss iteratively.\n",
        "\n",
        "**Important note**: The gradients need to be set to zero again before calling `.backward()` of another calculation involving `a` or `b`, otherwise they keep accumulating. \n",
        "\n",
        "*More detailed version*: \n",
        "$$\n",
        "d = d(\\vec{c}) = d(\\vec{c}(\\vec{a}, \\vec{b}))\\\\\n",
        "d(\\vec{c}) = c_0^2+c_1^2 = (c_0(\\vec{a}, \\vec{b}))^2 + (c_1(\\vec{a}, \\vec{b}))^2\\\\\n",
        "\\vec{c}(\\vec{a}, \\vec{b})=\\pmatrix{a_0 b_0 \\\\ a_1 b_1}\\\\\n",
        "\\frac{\\partial{d}}{\\partial{a_0}}{\\bigg|}_{a=[2, 3], b=[5, 1]} = \\frac{\\partial{(c_0^2+c_1^2)}}{\\partial{a_0}}{\\bigg|}_{a=[2, 3], b=[5, 1]} = \n",
        "\\frac{\\partial{(c_0^2)}}{\\partial{a_0}}+\\frac{\\partial{(c_1^2)}}{\\partial{a_0}}{\\bigg|}_{a=[2, 3], b=[5, 1]} = \\\\\n",
        "2c_0\\cdot \\frac{\\partial{c_0}}{\\partial{a_0}}+2c_1\\cdot \\frac{\\partial{c_1}}{\\partial{a_0}}{\\bigg|}_{a=[2, 3], b=[5, 1]} = \n",
        "2c_0\\cdot \\frac{\\partial{c_0}}{\\partial{a_0}}+0{\\bigg|}_{a=[2, 3], b=[5, 1]} = \n",
        "2c_0\\cdot \\frac{\\partial{(a_0\\cdot b_0)}}{\\partial{a_0}}{\\bigg|}_{a=[2, 3], b=[5, 1]} = \\\\\n",
        "(2c_0)\\cdot b_0 |_{a=[2, 3], b=[5, 1]} = (2 \\cdot (a_0 \\cdot b_0)) \\cdot b_0|_{a=[2, 3], b=[5, 1]} = (2\\cdot (2\\cdot 5))\\cdot 5 = 100 = \\mathrm{a.grad[0]}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlrrOH4KL1pE",
        "colab_type": "text"
      },
      "source": [
        "### Modules\n",
        "\n",
        "In PyTorch the `Module` class is a container which aggregates operations and the parameters involved. This makes it easy to create reusable building blocks. For example we can define a MLP module like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA3kKvd4L1pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    #create all child-modules and parameters inside the initialization method\n",
        "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
        "        super().__init__()\n",
        "        #weight parameters with random initialization\n",
        "        self.weights_hidden = torch.nn.Parameter(torch.randn((dim_in, dim_hidden), requires_grad=True))\n",
        "        self.biases_hidden = torch.nn.Parameter(torch.zeros(dim_hidden, requires_grad=True))\n",
        "        self.weights_out = torch.nn.Parameter(torch.randn((dim_hidden, dim_out), requires_grad=True))\n",
        "        self.biases_out = torch.nn.Parameter(torch.zeros(dim_out, requires_grad=True))\n",
        "        \n",
        "    #the forward method defines how the child-modules and parameters should be used on some input 'tensor_in'\n",
        "    #to generate the module output\n",
        "    def forward(self, tensor_in):\n",
        "        hidden_values = tensor_in.matmul(self.weights_hidden) + self.biases_hidden\n",
        "        hidden_activations = F.relu(hidden_values)\n",
        "        out_values = hidden_activations.matmul(self.weights_out) + self.biases_out\n",
        "        return out_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4ifIIPKL1pY",
        "colab_type": "text"
      },
      "source": [
        "To train an MLP using stochastic gradient descent we can use the SGD implementation included in `torch.optim`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwahQYwiL1pZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create an instance of the MLP with input size 3, 10 hidden neurons and 2 output neurons\n",
        "my_mlp = MLP(3, 10, 2)\n",
        "\n",
        "#create an optimizer instance and tell it which paramters to optimize\n",
        "optimizer = torch.optim.SGD(my_mlp.parameters(), lr=0.001)#learning rate of 0.001\n",
        "#optimizers have a convenience function to set all parameter gradients back to zero - although this would not be necessary before the first .backward() call\n",
        "optimizer.zero_grad()\n",
        "\n",
        "#get a sample\n",
        "input_tensor = torch.tensor([[1.5, 2.1, 5.1]])\n",
        "\n",
        "#define the loss function\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#using a module instance like a function invokes its \"forward\"  method\n",
        "prediction = my_mlp(input_tensor)\n",
        "\n",
        "#calculate the cross-entropy loss with the label for the one sample given above defined as \"1\"\n",
        "loss = loss_func(prediction, torch.tensor([1]))\n",
        "\n",
        "#perform back propagation\n",
        "loss.backward()\n",
        "\n",
        "#perform one SGD step using the gradients stored in the parameters\n",
        "optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzZPBQR3L1pn",
        "colab_type": "text"
      },
      "source": [
        "A list of built-in neural network modules, like convolution layers, can be found here: https://pytorch.org/docs/stable/nn.html#conv2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdgz8sKwL1po",
        "colab_type": "text"
      },
      "source": [
        "## EMNIST\n",
        "The Extended-MNIST dataset consists of about a million grayscale images of handwritten digits, uppercase letters and lowercase letters (62 classes) with a size of 28x28 pixels. It is pre-split into a training-set and a test-set. Here is an example from the training set and its corresponding label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2KNcPkjL1pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import string\n",
        "\n",
        "#create dataset object instances for the training and test set\n",
        "train_set = utils.EMNIST('data/emnist-byclass-train-images-idx3-ubyte', 'data/emnist-byclass-train-labels-idx1-ubyte')\n",
        "test_set = utils.EMNIST('data/emnist-byclass-test-images-idx3-ubyte', 'data/emnist-byclass-test-labels-idx1-ubyte')\n",
        "\n",
        "#select an arbitrary index as an example\n",
        "index = 689\n",
        "\n",
        "#retrieve the image and label from the dataset\n",
        "example_image, example_label = train_set.__getitem__(index)\n",
        "\n",
        "#flip axis to get the right format for imshow\n",
        "example_image = np.transpose(example_image)\n",
        "\n",
        "#invert black/white to make it look like black ink on white paper (just for viewing convenience)\n",
        "example_image = 255 - example_image\n",
        "\n",
        "#display the example image\n",
        "plt.imshow(example_image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "#build the class names list\n",
        "class_names = list(string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
        "\n",
        "#select the class name of the example's label\n",
        "print(class_names[example_label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaKRgoD4L1p5",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 5.1: Building a model for EMNIST classification\n",
        "Now let's build a model to predict the class labels on the EMNIST dataset. \n",
        "\n",
        "We want to start with a simple model with only two convolution layers (`torch.nn.Conv2d`) with max pooling (`torch.nn.functional.max_pool2d`) and a linear layer (`toch.nn.Linear`) to create the class probabilities.\n",
        "\n",
        "__Preparation__:\n",
        " - What is the width and height of the first convolution layer's output if it has a kernel size of 5 and no padding?\n",
        " - What is the width and height of the feature maps after the first pooling layer with a pooling size of 2?\n",
        " - What is the width and height of the feature maps after the second pooling layer?\n",
        " - If the second convolution layer generates 30 feature maps, how large is the input size of the linear layer?\n",
        " \n",
        "__Programming Hints__:\n",
        " - Use the documentation linked above to see the arguments of the `Conv2d` and `Linear` modules.\n",
        " - Functions in `torch.nn.functional` are really just functions, not modules, so you don't need to create instances of them in `__init__`. They can be applied to tensors directly in `forward`.\n",
        " - We will not feed the module single samples, which would be tensors with dimensions `(1, 28, 28)` (the `1` is the number of color channels), but batches of samples with dimensions `(batch_size, 1, 28, 28)`. The modules and functions in `torch.nn` are all capable of processing batches. \n",
        " - The loss function we use here (`torch.nn.CrossEntropyLoss`) implicitly applies softmax to the predictions so our model should not apply softmax to the output itself. Also since softmax is a monotonic function, finding the most likely class after softmax is equivalent to finding the highest scored entry before softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCeZONSML1p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        feature_maps_1 = 20\n",
        "        feature_maps_2 = 30\n",
        "        \n",
        "        num_classes = len(class_names)\n",
        "        \n",
        "        # TODO: initialize the child-modules\n",
        "        self.conv1 = \n",
        "        self.conv2 = \n",
        "\n",
        "        self.fully_connected = \n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: apply the modules and functions\n",
        "        feature_map_1_pre_activations = \n",
        "        feature_map_1_pre_pooling = \n",
        "        feature_map_1_post_pooling = \n",
        "        \n",
        "        feature_map_2_pre_activations = \n",
        "        feature_map_2_pre_pooling = \n",
        "        feature_map_2_post_pooling = \n",
        "        \n",
        "        # TODO: apply the final linear layer to get the class scores\n",
        "        feature_map_2_flattened = feature_map_2_post_pooling.flatten(start_dim=1)\n",
        "        predictions_pre_softmax = \n",
        "        \n",
        "        return predictions_pre_softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuof8YPmL1qM",
        "colab_type": "text"
      },
      "source": [
        "Now we want to train the model. You can try to tune the elements in the hyperparameter section to improve the test accuracy or change your model architecture. Training takes about 6500 batches per epoch for a batch size of 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUyTRiSmL1qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create an instance of our model\n",
        "model = MyModel()\n",
        "\n",
        "#Put the model on the GPU if available\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "    print(\"Training on GPU\")\n",
        "else:\n",
        "    print(\"Training on CPU\")\n",
        "\n",
        "###\n",
        "# Hyperparameters:\n",
        "###\n",
        "\n",
        "#Select a learning rate\n",
        "lr=0.001\n",
        "\n",
        "#Select an optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#Select a loss function\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#Select a training batch size\n",
        "batch_size = 100\n",
        "\n",
        "#Select the number of training epochs\n",
        "epochs = 1\n",
        "\n",
        "###\n",
        "# Training code:\n",
        "###\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#The DataLoader takes care of shuffling the dataset and serving batches\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)\n",
        "\n",
        "#Train for some epochs and calculate test accuracy once per epoch\n",
        "for epoch in range(epochs):\n",
        "    #test\n",
        "    utils.evaluate(epoch, model, loss_func, test_loader, use_gpu)\n",
        "    \n",
        "    #training\n",
        "    utils.train(epoch, model, loss_func, optimizer, train_loader, use_gpu)\n",
        "\n",
        "#Perform a final evaluation\n",
        "utils.evaluate(epochs, model, loss_func, test_loader, use_gpu)\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbdp4uprL1qr",
        "colab_type": "text"
      },
      "source": [
        "### Related questions\n",
        "The confusion matrix generated at the end of the training shows some distinctive features.\n",
        "1. What does the distribution of brightness on the diagonal of the confusion matrix say about the dataset?\n",
        "2. There is a faint diagonal line in the confusion matrix at $x \\in [10\\dots 36]$, $y \\in [36\\dots 62]$. What does it say about the labeling of the data?\n",
        "\n",
        "**Answers**:\n",
        "1. \n",
        "2. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH04wz4nL1qw",
        "colab_type": "text"
      },
      "source": [
        "To test your model's performance and robustness, here is an interactive canvas to draw inputs for the model to classify. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNR4SaxzL1q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "canvas_size = (280, 280)\n",
        "\n",
        "def predict_func(img_data):\n",
        "    #transform the list into a 2d array\n",
        "    img_data = np.asarray(img_data, dtype=np.float32).reshape(*canvas_size)\n",
        "    #transform the numpy array into a torch tensor\n",
        "    x = torch.tensor(img_data, dtype=torch.float)\n",
        "    #move it to the GPU is possible\n",
        "    if use_gpu:\n",
        "        x = x.cuda()\n",
        "    #scale it down from 280x280 to 28x28 pixels using bilinear interpolation\n",
        "    x = torch.nn.functional.interpolate(x.unsqueeze_(dim=0).unsqueeze_(dim=1), size=(28, 28), mode='bilinear', align_corners=True).squeeze_(dim=1).squeeze_(dim=0)\n",
        "    #flip dimensions to match dataset\n",
        "    x = x.transpose(0, 1)\n",
        "    #add a batch dimension with batch_size=1\n",
        "    x.unsqueeze_(dim=0)\n",
        "    #add a color dimension with color channel count=1\n",
        "    x.unsqueeze_(dim=1)\n",
        "    \n",
        "    #switch model to evaluation mode (only necessary for some modules like dropout and batch normalization, but better to always have it rather than forget it when needed)\n",
        "    model.eval()\n",
        "    #predict the label for the input\n",
        "    with torch.no_grad():#we don't want to store information for gradient computation\n",
        "        out = model(x)\n",
        "    #get the most likely label\n",
        "    pred_label = out.argmax(1)\n",
        "    pred_label = pred_label.item()\n",
        "    #return the predicted class name to the HTML-framework (to be displayed below)\n",
        "    return class_names[pred_label]\n",
        "\n",
        "#create the prediction canvas and make it use the prediction function defined above\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import output\n",
        "  output.register_callback('predict_func', predict_func)\n",
        "utils.make_prediction_canvas(canvas_size, \"predict_func\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg6FLpODL1rE",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 5.2: Hold my beer\n",
        "This exercise sheet is made for a span of two weeks. Try different hyperparameters and model architectures. What is the highest test accuracy you can achieve?\n",
        "\n",
        "Training is only allowed on the training data. No training on the test data or using models pretrained on other datasets.\n",
        "\n",
        "For portability you can save/load model weights using `torch.save(model.state_dict(), \"model_weights.pt\")` and `model.load_state_dict(torch.load(\"model_weights.pt\"))`."
      ]
    }
  ]
}