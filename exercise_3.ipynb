{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oBeNWXoVfG7w"
      },
      "source": [
        "# Exercise 3: Multilayer Perceptron (MLP)\n",
        "In this exercise, we will study a multilayer perceptron (MLP) with one hidden layer (comprising $M$ hidden neurons) and a single output neuron.\n",
        "\n",
        "We obtain the output of the MLP through *forward propagation* as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\boldsymbol{v}_{i} &= \\sigma_{\\beta} \\left( \\boldsymbol{\\hat{x}}_{i} \\boldsymbol{\\hat{W}}_{h}^{T}  \\right) \\\\\n",
        "    y_{i} &= \\sigma_{\\beta} \\left( \\boldsymbol{\\hat{v}}_{i} \\boldsymbol{\\hat{w}}_{o}^{T} \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where \n",
        "- $\\boldsymbol{\\hat{x}}_{i} \\in \\mathbb{R}^{1 \\times (N + 1)}$ is the extended `sample` $\\boldsymbol{x}_{i} \\in \\mathbb{R}^{1 \\times N}$,\n",
        "- $\\boldsymbol{\\hat{W}}_{h} \\in \\mathbb{R}^{M \\times (N + 1)}$ are the threshold extended `hidden_weights` of the hidden neurons,\n",
        "- $\\boldsymbol{v}_{i} \\in \\mathbb{R}^{1 \\times M}$ are the $M$ outputs of the hidden neurons, \n",
        "- $\\boldsymbol{\\hat{v}}_{i} \\in \\mathbb{R}^{1 \\times (M + 1)}$ is the extended hidden layer output vector,\n",
        "- $\\boldsymbol{\\hat{w}}_{o} \\in \\mathbb{R}^{1 \\times (M + 1)}$ are the threshold extended `output_weights` of the output neuron,\n",
        "- $y_{i} \\in \\mathbb{R}$ is the scalar output of the output neuron,\n",
        "- $\\sigma_{\\beta} \\left(\\cdot\\right) = \\text{tanh}\\left(\\frac{\\beta}{2}\\cdot\\right)$ is the perceptron `activation_function`.\n",
        "\n",
        "**Note**: The _threshold trick_ is applied, i.e. the threshold of each neuron is included as an additional _first_ component for each extended weight vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aO3hFUttfG70"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise. This mandatory preamble will be found on all exercise sheets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MA13VlBPfG77"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-luebeck/cs4405.git'\n",
        "    os.chdir('cs4405')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import utils_3 as utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ueMv0K7fG8O"
      },
      "source": [
        "## Exercise 3.1: Implementation of an MLP\n",
        "\n",
        "**Tasks**:\n",
        "- Implement the forward propagation of the MLP. \n",
        "- Your code should be flexible enough to allow an arbitrary $M \\in \\mathbb{N}^{+}$. \n",
        "\n",
        "**Programming Hints**:\n",
        "- The `hidden_weights` of the hidden neurons $\\boldsymbol{\\hat{W}}_{h}$, the `output_weights` of the output neuron $\\boldsymbol{\\hat{w}}_{o}$, and `beta` $\\beta$ should be passed to your function as parameters.\n",
        "- Try to avoid for-loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N79u7Pm6fG8R"
      },
      "outputs": [],
      "source": [
        "# TODO: implement the perceptron activation function\n",
        "def activation_function(x, beta):\n",
        "  pass\n",
        "\n",
        "# TODO: implement function for the forward propagation\n",
        "def classify_mlp(samples, hidden_weights, output_weights, beta):\n",
        "  \n",
        "  # TODO: n_samples: number of samples\n",
        "  n_samples = \n",
        "\n",
        "  # TODO: extend samples by '-1' column (threshold trick)\n",
        "  samples = \n",
        "\n",
        "  # TODO: compute outputs of the hidden neurons\n",
        "  hidden_outputs = \n",
        "\n",
        "  # TODO: extend hidden outputs by '-1' column (threshold trick)    \n",
        "  hidden_outputs = \n",
        "\n",
        "  # TODO: compute outputs of the output neurons\n",
        "  outputs = \n",
        "    \n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hotHnGvWfG8m"
      },
      "source": [
        "## Exercise 3.2: Classification with an MLP\n",
        "\n",
        "**Tasks**:\n",
        "- Test the forward propagation of the MLP and visualize the classification that you obtain for $1000$ points distributed uniformly in the region $\\left[ -1,1 \\right] \\times \\left[ -1,1 \\right]$\n",
        "- Experiment with different values of $\\beta$, including $3$, $5$, $20$, and $\\beta \\to \\infty$.\n",
        "\n",
        "**Programming Hints**:\n",
        "- Each of the loaded weight vectors contains a threshold as its first component.\n",
        "\n",
        "**Questions**:\n",
        "- What is the influence of $\\beta$ on the classification boundary?\n",
        "\n",
        "**Answers**:\n",
        "- \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SIBArNhCfG8o"
      },
      "outputs": [],
      "source": [
        "hidden_weights, output_weights = utils.load_data('data/data_3.npz')\n",
        "\n",
        "# TODO: uniformly create 2D random data in [-1,1] x [-1,1]\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "samples = \n",
        "\n",
        "# TODO: define the beta value (sigmoid function of the MLP)\n",
        "beta = \n",
        "\n",
        "# TODO: assign each output value >= 0 the class prediction +1 and each output value < 0 the class prediction -1\n",
        "outputs = classify_mlp(samples, hidden_weights, output_weights, beta)\n",
        "classifications = \n",
        "\n",
        "utils.plot_data(samples, classifications)\n",
        "utils.plot_classlines(hidden_weights)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "exercise_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
