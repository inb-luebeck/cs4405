{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "exercise_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROYUm36OwHDb",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 7: Recurrent Neural Networks (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chDjxkx5IdqX",
        "colab_type": "text"
      },
      "source": [
        "**Note**: Please insert the names of all participating students:\n",
        "1. \n",
        "2. \n",
        "3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvl01JubInQU",
        "colab_type": "text"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise. This mandatory preamble will be found on all exercise sheets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE1y8jUBI-wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-uni-luebeck/cs4405.git'\n",
        "    os.chdir('cs4405')\n",
        "\n",
        "#checking if data is unzipped and unzip if necessary\n",
        "if not os.path.isfile('data/ptb.train.txt'):\n",
        "    !unzip data/data_7.zip -d data/\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import utils_7 as utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYGMNzfRELMp",
        "colab_type": "text"
      },
      "source": [
        "## Language modelling\n",
        "A _Language Model_ is a model that takes an unfinished sequence of characters (in character-level language models) or words (in word-level language models) as input and predicts the next character or word token in the sequence. This approach is called semi-supervised because the labels (the upcoming word when given the string up to that point) are part of the training samples, so no extra labeling is required. \n",
        "\n",
        "In this exercise we will train a word-level language model on the PTB dataset using RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgekU4M-wHDj",
        "colab_type": "text"
      },
      "source": [
        "## RNNs in PyTorch\n",
        "PyTorch provides efficient implementations of a number of recurrent architectures, including the most prominent ones Gated Recurrent Units (GRUs) and Long Short Term Memory (LSTMs). \n",
        "\n",
        "While it is possible to manually build recurrent neural network architectures from basic tensor operations, it is usually much slower due to additional overhead and the sequential nature of RNNs. \n",
        "\n",
        "### The Penn Tree Bank (PTB) dataset\n",
        "The PTB dataset consists of sentences from newspaper articles. Numerals are replaced by just the capital letter \"N\" and any words outside the 10.000 most frequent ones are replaced with the \"`<unk>`\" token. This way all words are guaranteed to be frequent enough to allow a language model to generalize. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIGiXz-KwHDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the PTB dataset\n",
        "print('Loading dataset...')\n",
        "    \n",
        "ptb_train = utils.PTB(\"data/ptb.train.txt\")#training set\n",
        "ptb_valid = utils.PTB(\"data/ptb.valid.txt\")#validation set\n",
        "ptb_test = utils.PTB(\"data/ptb.test.txt\")#test set\n",
        "\n",
        "#determine the full set of words\n",
        "word_set = ptb_train.word_set.union(ptb_valid.word_set, ptb_test.word_set)\n",
        "\n",
        "dictionary = {\"<padding>\": 0}#dictionary to form network inputs from words\n",
        "inv_dictionary = {0: \"<padding>\"}#inverse dictionary to retrieve the actual words from network outputs\n",
        "#assign a dictionary index to every word\n",
        "for i, word in enumerate(word_set):\n",
        "    dictionary[word] = i+1\n",
        "    inv_dictionary[i+1] = word\n",
        "\n",
        "ptb_train.encode_sentences(dictionary)\n",
        "ptb_valid.encode_sentences(dictionary)\n",
        "ptb_test.encode_sentences(dictionary)\n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE6vITF7wHEJ",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 7.1: Padding of sequences\n",
        "RNNs are often used for sequence data like audio waves, texts in the form of character sequences or word sequences, or any type of time series. \n",
        "\n",
        "For good computational performance it's important to process a batch of samples at once rather than each sample individually. However, since each sample represents a sequence and sequences can vary in length, it's not possible to just stack them to form a batch tensor, so we need to apply padding to the sequences to make them the same length. \n",
        "\n",
        "The PTB class instantiated above is a Dataset object. It handles loading of single samples from the dataset but is not responsible for padding, batching or shuffling - that's the job of the `DataLoader`.\n",
        "\n",
        "The `DataLoader` selects sample indices for a batch, retrieves the corresponding samples from the dataset and calls the `collate` function to combine the list of samples into a batch.\n",
        "\n",
        "__Task__:\n",
        " - Implement the collate function \"`pad_and_batch_sequences`\" for the dataloaders below.\n",
        "\n",
        "__Programming Hints__:\n",
        " - Use [`torch.nn.utils.rnn.pad_sequence`](https://pytorch.org/docs/stable/nn.html#pad-sequence) for padding and use a padding value of `0` as defined in the dictionary above.\n",
        " - Depending on your choice `pad_sequence` will either use the first dimension as batch dimension and the second dimension as position index within the sequences or vice versa. For convenience we recommend sticking to the default `batch_first=False`, but it is important to keep in mind that the batch index then is the __second__ index of the `batch` tensor, not the first index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdm4FgwTwHEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# our custom collate function\n",
        "# input: list of sequences, expected output: batch of padded sequences\n",
        "def pad_and_batch_sequences(sequences):\n",
        "    #the \"sequences\" parameter is a list of 1-dimensional tensors (vectors) of different sizes\n",
        "    #each tensor represents a sequence, or more specifically, a sentence\n",
        "    #each tensor element is a number: the index of a word in the dictionary\n",
        "    \n",
        "    # TODO: sort the list of sequences by DESCENDING length (required in PyTorch 1.0, possibly unnecessary in later versions)\n",
        "    sequences\n",
        "    \n",
        "    # TODO: pad the sequences and combine them into a batch\n",
        "    batch = \n",
        "    \n",
        "    # TODO: calculate the original lengths of the sequences and store them in a tensor\n",
        "    lengths = \n",
        "    \n",
        "    return batch, lengths\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=ptb_train,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 collate_fn=pad_and_batch_sequences)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "                dataset=ptb_valid,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                collate_fn=pad_and_batch_sequences)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=ptb_test,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                collate_fn=pad_and_batch_sequences)\n",
        "\n",
        "#sanity check to test your implementation for errors\n",
        "sanity_check_sample1 = torch.tensor([1, 5])\n",
        "sanity_check_sample2 = torch.tensor([2, 4, 0, 3])\n",
        "sanity_check_batch = torch.tensor([[2, 1], [4, 5], [0, 0], [3, 0]])\n",
        "sanity_check_lengths = torch.tensor([4, 2])\n",
        "\n",
        "if (pad_and_batch_sequences([sanity_check_sample1, sanity_check_sample2])[0] != sanity_check_batch).any():\n",
        "  print('Sanity check failed for output \"batch\".')\n",
        "  print('Expected:\\n', sanity_check_batch)\n",
        "  print('Got:\\n', pad_and_batch_sequences([sanity_check_sample1, sanity_check_sample2])[0])\n",
        "elif (pad_and_batch_sequences([sanity_check_sample1, sanity_check_sample2])[1] != sanity_check_lengths).any():\n",
        "  print('Sanity check failed for output \"lengths\".')\n",
        "  print('Expected:\\n', sanity_check_lengths)\n",
        "  print('Got:\\n', pad_and_batch_sequences([sanity_check_sample1, sanity_check_sample2])[1])\n",
        "else:\n",
        "  print('Sanity checks passed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvVenawGwHFB",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 7.2: Building the model\n",
        "Let's start with a simple model with just a recurrent module and a linear layer to generate an output score for each dictionary entry. \n",
        "\n",
        "For the input of the network it has proven advantageous to represent every word by a vector rather than a single number. The values of the vectors can start out randomly and be learned and optimized through backpropagation during training. This is called embedding. The embedding vectors for each word in the dictionary are stored in an embedding matrix which is stored in the model. PyTorch provides an embedding-module with a convenient lookup implementation for this purpose. \n",
        "\n",
        "`Packing` allows you to pack the padded batch tensor tightly and to let the recurrent module know how long each of the sequences really are, so it doesn't need to calculate the outputs for the padding as well. PyTorch provides methods to create a PackedSequence from a padded tensor and vice versa.\n",
        "\n",
        "__Tasks__:\n",
        " - Add an embedding module, a recurrent layer and a linear layer to the model.\n",
        " - For each batch replace the word indices with the corresponding word embeddings.\n",
        " - Transform the batch tensor into a PackedSequence, run it through the recurrent layer and unpack its output, then apply the linear layer.\n",
        "\n",
        "__Preparation__:\n",
        " - Choose a type of recurrent layer from [the pytorch documentation](https://pytorch.org/docs/stable/nn.html#recurrent-layers). We recommend `LSTM` or `GRU`. \n",
        " - Get familiar with [`torch.nn.utils.rnn.pack_padded_sequence`](https://pytorch.org/docs/stable/nn.html#pack-padded-sequence) to create `PackedSequence` objects. Let the recurrent module work on `PackedSequence` objects rather than Tensors.\n",
        " \n",
        "__Programming Hints__:\n",
        " - Use the PyTorch module [`torch.nn.Embedding`](https://pytorch.org/docs/stable/nn.html#embedding) for the word embeddings. An instance of `torch.nn.Embedding` takes a tensor of indices and returns a tensor where every entry is replaced with the corresponding embedding-vector (so the result tensor also has one additional dimension of size `embedding_dim`). \n",
        " - Let the recurrent module have `2` layers.\n",
        " - The recurrent module can either take a non-packed `Tensor` and return its outputs and final hidden state as `Tensor` objects, or take a `PackedSequence` in which case the outputs object will also be a `PackedSequence`. Use the [`torch.nn.utils.rnn.pack_padded_sequence`](https://pytorch.org/docs/stable/nn.html#pack-padded-sequence) method to pack the padded batch before giving it to the recurrent layer and use [`torch.nn.utils.rnn.pad_packed_sequence`](https://pytorch.org/docs/stable/nn.html#pad-packed-sequence) to unpack the outputs of the recurrent layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy8pZwGcwHFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        embedding_dim = 1000\n",
        "        hidden_size = 1000\n",
        "        dict_size = len(dictionary)\n",
        "        \n",
        "        # TODO: instantiate an Embedding module\n",
        "        self.embedding = \n",
        "        \n",
        "        # TODO: give the model a recurrent module with 2 layers\n",
        "        self.rnn = \n",
        "        \n",
        "        # TODO: create a linear layer to generate the output that scores each entry of the dictionary\n",
        "        self.fc = \n",
        "        \n",
        "    def forward(self, batch, lengths):\n",
        "        # TODO: generate the tensor of embedding vectors from the tensor of word indices by applying the embedding module\n",
        "        batch_embedded = \n",
        "        \n",
        "        # TODO: pack the sequences tightly\n",
        "        batch_embedded_packed = \n",
        "        \n",
        "        # TODO: apply the RNN, note that it returns not just the outputs but also the hidden states\n",
        "        outputs, _ = \n",
        "        \n",
        "        # TODO: unpack the results by transforming them into a padded tensor again (pad_packed_sequence)\n",
        "        outputs_padded, _ = \n",
        "        \n",
        "        # TODO: apply the linear layer\n",
        "        predictions = \n",
        "        \n",
        "        return predictions\n",
        "###\n",
        "# Hyperparameters:\n",
        "###\n",
        "    \n",
        "#The starting learning rate\n",
        "lr=0.007\n",
        "\n",
        "#The factor by which the learning rate is decreased after each epoch\n",
        "lr_decay=0.6\n",
        "\n",
        "#The smallest value the learning rate can decay to\n",
        "lr_min=5e-4\n",
        "\n",
        "#The importance of the L2-regularization term - not actually weight decay for Adam optimizer implementation but still using the same name since it is equivalent to actual weight decay in the vanilla SGD optimizer\n",
        "weight_decay = 8e-6\n",
        "\n",
        "#sanity check to test your implementation for errors\n",
        "sanity_check_model = MyModel()\n",
        "sanity_check_sample1 = torch.tensor([12, 8])\n",
        "sanity_check_sample2 = torch.tensor([2, 411, 90, 31])\n",
        "sanity_check_batch, sanity_check_lengths = pad_and_batch_sequences([sanity_check_sample1, sanity_check_sample2])\n",
        "sanity_check_output_size = (max(sanity_check_sample1.shape[0], sanity_check_sample2.shape[0]), 2, len(dictionary))\n",
        "with torch.no_grad():\n",
        "  if tuple(sanity_check_model(sanity_check_batch, sanity_check_lengths).size()) != sanity_check_output_size:\n",
        "    print('Sanity check failed for model output size.')\n",
        "    print('Expected:\\n', sanity_check_output_size)\n",
        "    print('Got:\\n', tuple(sanity_check_model(sanity_check_batch, sanity_check_lengths).size()))\n",
        "  else:\n",
        "    print('Sanity check passed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzhiOr7_wHFi",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 7.3: Training the model\n",
        "For each sample sequence we can use the first word to predict the second, use the first two words to predict the third, use the first three words to predict the fourth, and so on. Each prediction is a classification problem where the number of classes is the dictionary size. The elements of the input sequence are the labels (except for the first element, since there is no input to predict it) and the outputs are the predicted classes (except for the last output, which goes beyond the last label). \n",
        "\n",
        "For each predicted next word we can calculate the cross entropy loss, using the actual next word as the classification label. The total loss for a batch is then the average loss over all the predictions of a sequence and over all the sequences of a batch. \n",
        "\n",
        "__Task__:\n",
        " - Implement the `train` and `evaluate` functions. Use the `model` to get the predictions. Extract the tensor of classification labels (`targets`) from the batch. Use the targets, the predictions and the `loss_func` to calculate the batch loss.\n",
        "\n",
        "__Preparation__:\n",
        " - Think carefully about which part of the input sequences and the network outputs can be used for the loss calculation and how they align.\n",
        "\n",
        "__Programming Hints__:\n",
        " - Remember that the batch index is the second index and the sequence index is the first index (unless you chose to use `batch_first=True` above). So `batch` and `predictions` both have shape (T, N, C) by default, where T is the length of the longest sequence (sentence) in the batch, N is the number of samples (sequences) in the batch (=batch_size) and C is the number of classes (number of words in the dictionary).\n",
        " - The loss function is set up to ignore predictions where the label is the padding value (see `ignore_index` further down), so the loss function can take padded tensors.\n",
        " - The loss function expects predictions of shape (N, C), where N is the number of predictions and C is the number of classes. And it expects targets of shape (N). So the dimension for the sequence index and the dimension for the batch index need to be combined into N. You can use `Tensor.flatten(start_dim, end_dim)` or `Tensor.reshape(*shape)`.\n",
        " - `Perplexity` is a commonly used measurement to evaluate a language model's performance. Roughly speaking it tells us how many words the model considers as candidates per predicion (on average). So for a completely untrained model with a dictionary size of 10.000 we would expect a perplexity of 10.000 and for a perfect model we would expect a perplexity of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWSnket0wHFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, dataloader, use_gpu, optimizer, loss_func):\n",
        "    model.train()\n",
        "    for i, (batch, lengths) in enumerate(dataloader):\n",
        "        if use_gpu:\n",
        "            #move the batch to gpu memory\n",
        "            batch = batch.cuda()\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # TODO: get the predictions\n",
        "        out = \n",
        "        \n",
        "        # TODO: get the part of the batch that should be used as labels\n",
        "        targets = \n",
        "\n",
        "        #sanity check\n",
        "        assert targets.numel() == max(0, batch.shape[0] - 1) * batch.shape[1], \"Sanity check failed for size of 'targets'. (remove check/switch indices if you use batch_first=True)\"\n",
        "        \n",
        "        # TODO: calculate the loss using loss_func (predictions)\n",
        "        loss = \n",
        "\n",
        "        #back propagation\n",
        "        loss.backward()\n",
        "        \n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "\n",
        "        #learning step\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i % 100 == 0 and use_gpu) or (i % 5 == 0 and not use_gpu):\n",
        "            perplexity = loss.exp()\n",
        "            print(\"training perplexity:\", perplexity)\n",
        "            \n",
        "def evaluate(model, dataloader, use_gpu, optimizer, loss_func):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for i, (batch, lengths) in enumerate(dataloader):\n",
        "            if use_gpu:\n",
        "                #move the batch to gpu memory\n",
        "                batch = batch.cuda()\n",
        "            \n",
        "            # TODO: get the predictions\n",
        "            out = \n",
        "            \n",
        "            # TODO: get the part of the batch that should be used as labels\n",
        "            targets = \n",
        "            \n",
        "            # TODO: calculate the loss\n",
        "            loss = \n",
        "\n",
        "            losses.append(loss)\n",
        "\n",
        "        perplexity = torch.stack(losses, dim=0).mean().exp()\n",
        "\n",
        "        print('evaluation perplexity:', perplexity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGelIEdewHFy",
        "colab_type": "text"
      },
      "source": [
        "Let's train the model. If everything is set up correctly, the perplexity should go well below 1000 within the first epoch.\n",
        "\n",
        "__Note__: The model starts overfitting after a few epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjVY0H6bwHF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MyModel()\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {\"params\": model.parameters(), \"weight_decay\": weight_decay},\n",
        "    ], lr=lr)\n",
        "\n",
        "# 0 is the <padding> word index as defined in dictionary[\"<padding>\"] = 0 and conversely inv_dictionary[0] = \"<padding>\" above\n",
        "loss_func = torch.nn.CrossEntropyLoss(ignore_index=0)#ignore targets in the padding section (label=0)\n",
        "\n",
        "epochs = 4\n",
        "for epoch in range(epochs):\n",
        "    print(\"epoch \" + str(epoch))\n",
        "    \n",
        "    train(model, train_loader, use_gpu, optimizer, loss_func)\n",
        "    \n",
        "    evaluate(model, valid_loader, use_gpu, optimizer, loss_func)\n",
        "    \n",
        "    lr *= lr_decay\n",
        "    if lr < lr_min:\n",
        "        lr = lr_min\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "        \n",
        "    torch.save((model.state_dict(), dictionary), \"model\" + str(epoch) + \".pt\")\n",
        "        \n",
        "print(\"Test set perplexity:\")\n",
        "evaluate(model, test_loader, use_gpu, optimizer, loss_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMqOX0_JwHGH",
        "colab_type": "text"
      },
      "source": [
        "### Trying the language model\n",
        "You can use the textbox below to get predictions from your model. Use `backspace` while the textbox is empty to remove word tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dboafVTRwHGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_weights = False\n",
        "\n",
        "if not load_weights:\n",
        "    #to test the model from this notebook:\n",
        "    trained_model, original_dictionary, original_inv_dictionary = model, dictionary, inv_dictionary\n",
        "else:\n",
        "    #to test the model from this notebook from stored weights:\n",
        "    model_state_dict, original_dictionary = torch.load(\"model.pt\", map_location=\"cpu\")\n",
        "    original_inv_dictionary = {v: k for k, v in original_dictionary.items()}\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    trained_model = model\n",
        "    \n",
        "batch_first = False #False unless you explicitly specified batch_first=true in the packing, padding and RNN functions and implemented the loss calculation accordingly\n",
        "\n",
        "if use_gpu:\n",
        "    trained_model = trained_model.cuda()\n",
        "\n",
        "def predict_func(sentence):#sentence as list of word strings\n",
        "    #max number of additional words to predict\n",
        "    max_len = 50\n",
        "    for _ in range(max_len):\n",
        "        #create a tensor from the words' dictionary indices\n",
        "        input_sentence = torch.tensor([original_dictionary[word] for word in sentence])\n",
        "        lengths = torch.tensor([input_sentence.size()[0]])\n",
        "        \n",
        "        #introduce the singular batch dimension\n",
        "        if batch_first:\n",
        "            input_sentence = input_sentence.unsqueeze(dim=0)\n",
        "        else:\n",
        "            input_sentence = input_sentence.unsqueeze(dim=1)\n",
        "            \n",
        "        if use_gpu:\n",
        "            input_sentence = input_sentence.cuda()\n",
        "        \n",
        "        #use the language model to predict the most likely next word\n",
        "        trained_model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = trained_model(input_sentence, lengths)\n",
        "        \n",
        "        #ignore predictions of the placeholder for rare words '<unk>' (dictionary limited to the 10k most common words in the dataset)\n",
        "        if batch_first:\n",
        "            out[0, -1, original_dictionary['<unk>']] = 0\n",
        "            out = out.argmax(-1)[0,-1]\n",
        "        else:\n",
        "            out[-1, 0, original_dictionary['<unk>']] = 0\n",
        "            out = out.argmax(-1)[-1,0]\n",
        "        \n",
        "        #get the string representation of the predicted word\n",
        "        out_word = original_inv_dictionary[out.item()]\n",
        "        \n",
        "        #append word to sentence word list\n",
        "        sentence.append(out_word)\n",
        "        \n",
        "        #stop predictions if end-of-sentence was predicted\n",
        "        if out_word == '<eos>': break\n",
        "            \n",
        "    #return the sentence as a single string\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "#create the input field with autocomplete for the available dictionary (autocomplete only shows suggestions if the number of matching suggestions is not too large)\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import output\n",
        "  output.register_callback('predict_func', predict_func)\n",
        "utils.make_prediction_field(original_dictionary, \"predict_func\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}